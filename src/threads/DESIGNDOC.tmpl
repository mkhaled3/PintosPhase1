            +--------------------+
            |        CS 140      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Mohamed Khaled es-mohamed.khaled.abbas20@alexu.edu.eg
Amr es-Amr.Elkhatieb2024@alexu.edu.eg
Mahmoud Mohamed es-mahmoudm.abd-elaziz2024@alexu.edu.eg
Moamen Mahmoud es-momengharib22@alexu.edu.eg

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/*
timer.c: global:
struct list sleeping_threads_list       /* List to hold sleeping threads, sorted so the least thread with wake up ticks is on the beining of the list. */
struct lock lock_sleeping_threads_list  /* lock to synchronize editing the sleeping_threads_list */

thread.h: struct thread:
int64_t wake_up_ticks                   /* variable to hold the thread wake up tick in case if it was put into sleep */
struct list_elem sleeping_elem          /* list element to be used storing the thread in "sleeping threads list" in order to be iterated and waken up */
*/

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

/*
- Make sure that ticks to sleep isn't either zero or negative (instant wake up).
- acquire the lock_sleeping_threads_list avoide many threads overlap reading/writing the list.
- Set the thread wake up ticks and insert it in its decreasing order to the sleaping threads list.
- Disable the interrupt to block the thread to put it into sleep.
- (Note we are inserting the threads in order to avoide much overhead at interrupt handlers when waking them up,
    so the interrupt handler will check if its time to wake the first thread in the list, if so wake the thread up and,
    check the next thread, else do nothing)
*/

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

/*
- We are inserting the threads in the sleeping_threads_list in order by their wake_up_ticks to avoide much overhead at interrupt handlers when waking them up,
    so the interrupt handler will check if its time to wake the first thread in the list, if so wake the thread up and,
    check the next thread, else do nothing.
*/

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

/*
- Only one thread who acquired the lock_sleeping_threads_list will edit the sleeping_threads_list, the rest will wait on the lock
*/

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

/*
- In timer_interrupt, Check if the sleeping threads list is being edited (a thread is holding the lock), if so then return
*/

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

/*
- Using synchronization tools like the lock_sleeping_threads_list rather than disabling the interrupts avoide halting the system.
- Inserting the threads in order by their wake_up_ticks avoid much overhead at timer_interrupt iterating the whole list sellecting first thread to be waken up.
*/

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/*
synch.h: struct lock:
struct list_elem myLock            /* list element to add the lock into a list (thread aquired locks list) used in priority donation */
int priority                       /* Priority variable to be used in priority donation (for sorting locks in thread aquired locks list) */

synch.c: struct semaphore_elem:
int *priority                      /* pointer for the waiting thread priority, used to unblock the highest priority thread on condvars */
*/

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

/*
priority is donated to the locks so there is no distinct data structure to track the donations, how the priority is donated is explained in details in question b4 and b5
png file is added															
*/

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

/*
- at semaphores we iterate the waiters list using list_max to sellect the highest priority thread to be waken up,
    if the sellected thread's priority higher than the current working thread yield the processor (reschedule).
- locks are using the semaphores so they face no problems.
- at condvars each semaphore has a pointer pointing to the waiting thread priority so we iterate the waiters list using list_max to sellect highest priority thread.
*/

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

/*
First call the lock_try_aquire function if the return value is true the lock holder gets the priority of the lock if the lock's priority is higher than the thread's priority
if the threads priority is higher the lock takes the thread's priority and then we insert that lock into the list of aquired locks of that thread
if the return value is false the thread's waiting on lock is set to this lock and then we repeat the steps of checking whose priority is higher the lock or the thread
and then we call nested_donation function 
nested_donation is a recursive function that checks if the thread priority is higher than the waiting on lock priority if so let the lock priority = the thread priority and the
holder of that lock takes that priority and then we call it again with the holder of that lock becuase it should donate to other threads that aquired a lock that this thread is
waiting on we keep calling it untill the waiting on lock is null or the thread is null and then when we return from that recursive call we call sema down and set the holder of
the lock to this thread and set the waiting on lock to null
*/

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

/*
first remove the lock from the list of the threads aquired locks
the lock takes the highest priority among the waitiers on it
if there are no waitiers the locks priority is set to zero

if the thread that released the lock has other locks in the aquired locks list we check if the highest priority lock is higher than this thread base priority if so
we set the priority of that thread to the highest priority lock
otherwise we set the thread's priority to the base priority
*/

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

/*
- if the thread has no aquired locks or if the new priority is higher than the highest priority aquired lock,
    change the thread working and original priority to the new priority,
    else just change the thread original priority because the working priority is needed for donation
- Disable the interrupts to avoide multiple threads overlapping reading/editing the ready list
- Sellect the highest priority thread in the ready list, if its priority is higher than the current working one, reschedule
*/

/*
Note that we are disabling the interrupts and we can't use locks, because if an interrupt happened while that thread is holding the lock,
    another thread might work and wait for the same lock causing an infinity loop of reschedulting.
*/

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

/*
- using list_max insted of list_insert_ordered when sellecting highes priority thread, lock, or semaphore prevents us from
    reordring the lists each time a thread change its priority.
- using a pointer in condition variables to point for the threads priority prevents us from changing the semaphore_elem priority each time the thread change its priority.
- avoid using unnecessary busy waiting, synchronization tools helps to increase the performance.
*/

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> In struct thread: 
		- Added two new fields: 
			1- int nice; 
				Represents the nice value of the thread. 
			2- int recent_cpu; 
				Represents recent_cpu value of the thread. 
	    - Added new global static variable to thread.c 
			static int load_avg; 

---- ALGORITHMS ----

>> Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer   recent_cpu   priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59      A
 4      4   0   0  62  61  59      A
 8      8   0   0  61  61  59      B  
 12     8   4   0  61  60  59      A
 16     12  4   0  60  60  59      B
 20     12  8   0  60  59  59      A
 24     16  8   0  59  59  59      C
 28     16  8   4  59  59  58      B
 32     16  12  4  59  58  58      A
 36     20  12  4  58  58  58      C 

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

/*- Yes, there was little ambiguity, like the one that appears in table above at tick 8, 
If a ready thread has same priority as the current thread, should yield be called, 
or the current thread continues to the second time slice. 
Since keeping the current thread will cause starvation for ready threads, this 
was easily handled by calling  list_max (&ready_list, &less_thread_priority_comp, NULL); inside 
next_thread_to_run function, so the next running thread is the one having maximum priority with function 
less_thread_priority_comp, which guarantees that the next thread to run is the one having the same 
priority as the running one, not the running thread
Note: <<You can find in "pintos/src/threads/" a folder called snippets containing some snippets of the output of some
self written tests with some printfs, indicating that the above criteria of thread scheduling is the one happening in 
our code.>>  
-*/ 

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?
	
/*- Code of calculating performance is in thread.c and divided to five functions 
increment_recent_cpu();
thread_foreach(update_recent_cpu, NULL);
update_load_avg();
thread_foreach(update_priority, NULL);
resort_ready_threads();
however, resort_ready_threads() tends to slow the code a little bit, if the size of ready threads list
is quite big. 
-*/
	
---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

/*- Advantages: One task functions: Each function in mlfqs calculations has only one calculation to get which makes code more readable and modular. 
	worth_yield() -> Checks if the current thread no longer has the highest priority
	update_recent_cpu(struct thread *t) -> Update recent cpu for a thread gets called every (1 sec)
	update_load_avg() -> function to update load_avg of the system
	update_priority(struct thread *t) -> function to update priority for a single thread
	increment_recent_cpu() -> increments recent_cpu for current thread
	
	Taking advantage of built in list functions: list_sort, list_foreach 
	
	
	Disadvantages: 
	- Used 1 queue for advanced scheduler, Using 64 queue will improve its complexity. 
	for example: 
	in resort ready threads functions, list_sort is called, assuming that complexity of sorting is (nlog(n)) 
	and that number of ready threads is alittle close to 64, we can assume that there's 1 or 2 threads in each 
	queue

		∑lni from i = 0 to n 
		equals lnn!
	which can be proved to be less than C*nlogn. 

	- Resort_ready_threads() tends to slow the code a little bit, maybe a better algorithm 
	is to resort ready list on bigger intervals than every 4 ticks, and through this second 
	choosing thread to run will random, but once the second is complete, list is sorted again. 
	
	
	If I have extra time to work: 
	- Apply sorting on big intervals, and choosing another scheduling algorithm that 
	doesn't require sorting during this interval. 
	- Apply 64 queue method. 
	-*/


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

/*-It was implemented using shifting, left shifting operator <<, each int is 
shifted 14 bits to the left, 17 bits for whole part, 14 bits for fraction part. 
 It is simple method, and efficient since shift operations are very fast. -*/


               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
